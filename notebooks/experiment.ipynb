{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f0effeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from loguru import logger\n",
    "from datetime import timedelta\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b5ddcbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///app/mlflow/artifacts/1', creation_time=1764185381721, experiment_id='1', last_update_time=1764185381721, lifecycle_stage='active', name='mobile-item-recommendation', tags={}>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLFLOW_EXPERIMENT_NAME = \"mobile-item-recommendation\"\n",
    "\n",
    "# Fix MLflow trash directory issue\n",
    "import os\n",
    "mlruns_path = os.path.join(os.getcwd(), \"mlruns\")\n",
    "trash_path = os.path.join(mlruns_path, \".trash\")\n",
    "os.makedirs(trash_path, exist_ok=True)\n",
    "\n",
    "# Optionally connect to Docker MLflow server (uncomment if running in Docker)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "# mlflow.set_tracking_uri(\"/Users/duc.tran/mobile-item-recommendation/notebooks/mlruns\")\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ead65a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/duc.tran/mobile-item-recommendation/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "051658ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/duc.tran/mobile-item-recommendation/data/raw\"\n",
    "RAW_USER_DATA_PATH = os.path.join(DATA_PATH, \"tianchi_fresh_comp_train_user_online_partA.txt\")\n",
    "RAW_ITEM_DATA_PATH = os.path.join(DATA_PATH, \"tianchi_fresh_comp_train_item_online.txt\")\n",
    "OUTPUT_FILE = os.path.join(DATA_PATH, \"items.parquet\")\n",
    "SAMPLE_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52b9744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw data (Sample Rate: 10.0%)...\n",
      "Item Subset Size: 6052459\n",
      "Processed chunk... current accumulated rows: 5539\n",
      "Processed chunk... current accumulated rows: 13399\n",
      "Processed chunk... current accumulated rows: 20686\n",
      "Processed chunk... current accumulated rows: 31035\n",
      "Processed chunk... current accumulated rows: 38090\n",
      "Processed chunk... current accumulated rows: 52333\n",
      "Processed chunk... current accumulated rows: 61230\n",
      "Processed chunk... current accumulated rows: 67666\n",
      "Processed chunk... current accumulated rows: 77156\n",
      "Processed chunk... current accumulated rows: 84391\n",
      "Processed chunk... current accumulated rows: 90976\n",
      "Processed chunk... current accumulated rows: 98515\n",
      "Processed chunk... current accumulated rows: 105672\n",
      "Processed chunk... current accumulated rows: 114319\n",
      "Processed chunk... current accumulated rows: 122493\n",
      "Processed chunk... current accumulated rows: 133179\n",
      "Processed chunk... current accumulated rows: 142823\n",
      "Processed chunk... current accumulated rows: 152929\n",
      "Processed chunk... current accumulated rows: 164032\n",
      "Processed chunk... current accumulated rows: 172755\n",
      "Processed chunk... current accumulated rows: 180930\n",
      "Processed chunk... current accumulated rows: 193316\n",
      "Processed chunk... current accumulated rows: 203908\n",
      "Processed chunk... current accumulated rows: 215283\n",
      "Processed chunk... current accumulated rows: 223733\n",
      "Processed chunk... current accumulated rows: 233576\n",
      "Processed chunk... current accumulated rows: 243822\n",
      "Processed chunk... current accumulated rows: 250983\n",
      "Processed chunk... current accumulated rows: 260176\n",
      "Processed chunk... current accumulated rows: 269515\n",
      "Processed chunk... current accumulated rows: 280247\n",
      "Processed chunk... current accumulated rows: 286874\n",
      "Processed chunk... current accumulated rows: 295954\n",
      "Processed chunk... current accumulated rows: 303963\n",
      "Processed chunk... current accumulated rows: 311757\n",
      "Processed chunk... current accumulated rows: 321178\n",
      "Processed chunk... current accumulated rows: 330825\n",
      "Processed chunk... current accumulated rows: 338739\n",
      "Processed chunk... current accumulated rows: 346135\n",
      "Processed chunk... current accumulated rows: 353780\n",
      "Processed chunk... current accumulated rows: 361505\n",
      "Processed chunk... current accumulated rows: 370108\n",
      "Processed chunk... current accumulated rows: 381490\n",
      "Processed chunk... current accumulated rows: 393038\n",
      "Processed chunk... current accumulated rows: 403195\n",
      "Processed chunk... current accumulated rows: 415482\n",
      "Processed chunk... current accumulated rows: 425030\n",
      "Processed chunk... current accumulated rows: 431812\n",
      "Processed chunk... current accumulated rows: 446390\n",
      "Processed chunk... current accumulated rows: 458535\n",
      "Processed chunk... current accumulated rows: 473087\n",
      "Processed chunk... current accumulated rows: 481516\n",
      "Processed chunk... current accumulated rows: 489262\n",
      "Processed chunk... current accumulated rows: 497849\n",
      "Processed chunk... current accumulated rows: 504744\n",
      "Processed chunk... current accumulated rows: 513929\n",
      "Processed chunk... current accumulated rows: 522345\n",
      "Processed chunk... current accumulated rows: 532470\n",
      "Processed chunk... current accumulated rows: 540960\n",
      "Processed chunk... current accumulated rows: 549957\n",
      "Processed chunk... current accumulated rows: 557199\n",
      "Processed chunk... current accumulated rows: 566614\n",
      "Processed chunk... current accumulated rows: 576237\n",
      "Processed chunk... current accumulated rows: 583949\n",
      "Processed chunk... current accumulated rows: 593243\n",
      "Processed chunk... current accumulated rows: 600246\n",
      "Processed chunk... current accumulated rows: 607858\n",
      "Processed chunk... current accumulated rows: 614641\n",
      "Processed chunk... current accumulated rows: 623450\n",
      "Processed chunk... current accumulated rows: 631914\n",
      "Processed chunk... current accumulated rows: 642491\n",
      "Processed chunk... current accumulated rows: 654766\n",
      "Processed chunk... current accumulated rows: 663355\n",
      "Processed chunk... current accumulated rows: 673179\n",
      "Processed chunk... current accumulated rows: 682685\n",
      "Processed chunk... current accumulated rows: 692275\n",
      "Processed chunk... current accumulated rows: 701199\n",
      "Processed chunk... current accumulated rows: 712358\n",
      "Processed chunk... current accumulated rows: 722382\n",
      "Processed chunk... current accumulated rows: 730587\n",
      "Processed chunk... current accumulated rows: 738955\n",
      "Processed chunk... current accumulated rows: 746900\n",
      "Processed chunk... current accumulated rows: 754210\n",
      "Processed chunk... current accumulated rows: 762321\n",
      "Processed chunk... current accumulated rows: 769527\n",
      "Processed chunk... current accumulated rows: 778045\n",
      "Processed chunk... current accumulated rows: 785119\n",
      "Processed chunk... current accumulated rows: 792683\n",
      "Processed chunk... current accumulated rows: 800502\n",
      "Processed chunk... current accumulated rows: 810101\n",
      "Processed chunk... current accumulated rows: 818929\n",
      "Processed chunk... current accumulated rows: 826624\n",
      "Processed chunk... current accumulated rows: 834421\n",
      "Processed chunk... current accumulated rows: 842990\n",
      "Processed chunk... current accumulated rows: 849799\n",
      "Processed chunk... current accumulated rows: 860302\n",
      "Processed chunk... current accumulated rows: 866819\n",
      "Processed chunk... current accumulated rows: 879419\n",
      "Processed chunk... current accumulated rows: 888625\n",
      "Processed chunk... current accumulated rows: 900175\n",
      "Processed chunk... current accumulated rows: 912090\n",
      "Processed chunk... current accumulated rows: 922957\n",
      "Processed chunk... current accumulated rows: 928787\n",
      "Processed chunk... current accumulated rows: 937008\n",
      "Processed chunk... current accumulated rows: 945788\n",
      "Processed chunk... current accumulated rows: 960442\n",
      "Processed chunk... current accumulated rows: 970838\n",
      "Processed chunk... current accumulated rows: 979448\n",
      "Processed chunk... current accumulated rows: 988125\n",
      "Processed chunk... current accumulated rows: 993974\n",
      "Processed chunk... current accumulated rows: 1000524\n",
      "Processed chunk... current accumulated rows: 1007804\n",
      "Processed chunk... current accumulated rows: 1016546\n",
      "Processed chunk... current accumulated rows: 1029586\n",
      "Processed chunk... current accumulated rows: 1038225\n",
      "Processed chunk... current accumulated rows: 1044953\n",
      "Processed chunk... current accumulated rows: 1052339\n",
      "Processed chunk... current accumulated rows: 1062316\n",
      "Processed chunk... current accumulated rows: 1071532\n",
      "Processed chunk... current accumulated rows: 1079619\n",
      "Processed chunk... current accumulated rows: 1089093\n",
      "Processed chunk... current accumulated rows: 1097232\n",
      "Processed chunk... current accumulated rows: 1109551\n",
      "Processed chunk... current accumulated rows: 1114103\n",
      "Processed chunk... current accumulated rows: 1124611\n",
      "Processed chunk... current accumulated rows: 1131517\n",
      "Processed chunk... current accumulated rows: 1138554\n",
      "Processed chunk... current accumulated rows: 1149344\n",
      "Processed chunk... current accumulated rows: 1157921\n",
      "Processed chunk... current accumulated rows: 1170556\n",
      "Processed chunk... current accumulated rows: 1176050\n",
      "Processed chunk... current accumulated rows: 1187173\n",
      "Processed chunk... current accumulated rows: 1194981\n",
      "Processed chunk... current accumulated rows: 1206632\n",
      "Processed chunk... current accumulated rows: 1215478\n",
      "Processed chunk... current accumulated rows: 1224062\n",
      "Processed chunk... current accumulated rows: 1230503\n",
      "Processed chunk... current accumulated rows: 1237964\n",
      "Processed chunk... current accumulated rows: 1245552\n",
      "Processed chunk... current accumulated rows: 1252803\n",
      "Processed chunk... current accumulated rows: 1261540\n",
      "Processed chunk... current accumulated rows: 1270138\n",
      "Processed chunk... current accumulated rows: 1281662\n",
      "Processed chunk... current accumulated rows: 1288837\n",
      "Processed chunk... current accumulated rows: 1298742\n",
      "Processed chunk... current accumulated rows: 1307668\n",
      "Processed chunk... current accumulated rows: 1316439\n",
      "Processed chunk... current accumulated rows: 1323616\n",
      "Processed chunk... current accumulated rows: 1334294\n",
      "Processed chunk... current accumulated rows: 1342475\n",
      "Processed chunk... current accumulated rows: 1349183\n",
      "Processed chunk... current accumulated rows: 1365419\n",
      "Processed chunk... current accumulated rows: 1375822\n",
      "Processed chunk... current accumulated rows: 1383065\n",
      "Processed chunk... current accumulated rows: 1392968\n",
      "Processed chunk... current accumulated rows: 1402970\n",
      "Processed chunk... current accumulated rows: 1412200\n",
      "Processed chunk... current accumulated rows: 1420959\n",
      "Processed chunk... current accumulated rows: 1429611\n",
      "Processed chunk... current accumulated rows: 1438259\n",
      "Processed chunk... current accumulated rows: 1452463\n",
      "Processed chunk... current accumulated rows: 1463142\n",
      "Processed chunk... current accumulated rows: 1472445\n",
      "Processed chunk... current accumulated rows: 1484020\n",
      "Processed chunk... current accumulated rows: 1490894\n",
      "Processed chunk... current accumulated rows: 1501175\n",
      "Processed chunk... current accumulated rows: 1510496\n",
      "Processed chunk... current accumulated rows: 1522817\n",
      "Processed chunk... current accumulated rows: 1533127\n",
      "Processed chunk... current accumulated rows: 1552232\n",
      "Processed chunk... current accumulated rows: 1559800\n",
      "Processed chunk... current accumulated rows: 1572693\n",
      "Processed chunk... current accumulated rows: 1581522\n",
      "Processed chunk... current accumulated rows: 1591511\n",
      "Processed chunk... current accumulated rows: 1600952\n",
      "Processed chunk... current accumulated rows: 1607895\n",
      "Processed chunk... current accumulated rows: 1615741\n",
      "Processed chunk... current accumulated rows: 1624729\n",
      "Processed chunk... current accumulated rows: 1632278\n",
      "Processed chunk... current accumulated rows: 1643821\n",
      "Processed chunk... current accumulated rows: 1655089\n",
      "Processed chunk... current accumulated rows: 1665968\n",
      "Processed chunk... current accumulated rows: 1677738\n",
      "Processed chunk... current accumulated rows: 1689477\n",
      "Processed chunk... current accumulated rows: 1699729\n",
      "Processed chunk... current accumulated rows: 1708099\n",
      "Processed chunk... current accumulated rows: 1715754\n",
      "Processed chunk... current accumulated rows: 1723040\n",
      "Processed chunk... current accumulated rows: 1734334\n",
      "Processed chunk... current accumulated rows: 1745832\n",
      "Processed chunk... current accumulated rows: 1757791\n",
      "Processed chunk... current accumulated rows: 1761680\n",
      "Processed chunk... current accumulated rows: 1770161\n",
      "Processed chunk... current accumulated rows: 1776195\n",
      "Processed chunk... current accumulated rows: 1783100\n",
      "Processed chunk... current accumulated rows: 1795986\n",
      "Processed chunk... current accumulated rows: 1805271\n",
      "Processed chunk... current accumulated rows: 1812816\n",
      "Processed chunk... current accumulated rows: 1823879\n",
      "Processed chunk... current accumulated rows: 1832508\n",
      "Processed chunk... current accumulated rows: 1845228\n",
      "Processed chunk... current accumulated rows: 1857481\n",
      "Processed chunk... current accumulated rows: 1867318\n",
      "Processed chunk... current accumulated rows: 1876520\n",
      "Processed chunk... current accumulated rows: 1884344\n",
      "Processed chunk... current accumulated rows: 1892598\n",
      "Processed chunk... current accumulated rows: 1900724\n",
      "Processed chunk... current accumulated rows: 1910770\n",
      "Processed chunk... current accumulated rows: 1919970\n",
      "Processed chunk... current accumulated rows: 1928489\n",
      "Processed chunk... current accumulated rows: 1938665\n",
      "Processed chunk... current accumulated rows: 1947089\n",
      "Processed chunk... current accumulated rows: 1953880\n",
      "Processed chunk... current accumulated rows: 1963264\n",
      "Processed chunk... current accumulated rows: 1970674\n",
      "Processed chunk... current accumulated rows: 1980165\n",
      "Processed chunk... current accumulated rows: 1990022\n",
      "Processed chunk... current accumulated rows: 2000688\n",
      "Processed chunk... current accumulated rows: 2010084\n",
      "Processed chunk... current accumulated rows: 2024469\n",
      "Processed chunk... current accumulated rows: 2035880\n",
      "Processed chunk... current accumulated rows: 2042530\n",
      "Processed chunk... current accumulated rows: 2050502\n",
      "Processed chunk... current accumulated rows: 2060295\n",
      "Processed chunk... current accumulated rows: 2070280\n",
      "Processed chunk... current accumulated rows: 2079128\n",
      "Processed chunk... current accumulated rows: 2085259\n",
      "Processed chunk... current accumulated rows: 2093844\n",
      "Processed chunk... current accumulated rows: 2104345\n",
      "Processed chunk... current accumulated rows: 2113927\n",
      "Processed chunk... current accumulated rows: 2124204\n",
      "Processed chunk... current accumulated rows: 2130948\n",
      "Processed chunk... current accumulated rows: 2140065\n",
      "Processed chunk... current accumulated rows: 2145170\n",
      "Processed chunk... current accumulated rows: 2153024\n",
      "Processed chunk... current accumulated rows: 2165820\n",
      "Processed chunk... current accumulated rows: 2174623\n",
      "Processed chunk... current accumulated rows: 2184395\n",
      "Processed chunk... current accumulated rows: 2194691\n",
      "Processed chunk... current accumulated rows: 2203499\n",
      "Processed chunk... current accumulated rows: 2212647\n",
      "Processed chunk... current accumulated rows: 2222575\n",
      "Processed chunk... current accumulated rows: 2231506\n",
      "Processed chunk... current accumulated rows: 2240529\n",
      "Processed chunk... current accumulated rows: 2249244\n",
      "Processed chunk... current accumulated rows: 2255504\n",
      "Processed chunk... current accumulated rows: 2264051\n",
      "Processed chunk... current accumulated rows: 2272175\n",
      "Processed chunk... current accumulated rows: 2281711\n",
      "Processed chunk... current accumulated rows: 2291892\n",
      "Processed chunk... current accumulated rows: 2301033\n",
      "Processed chunk... current accumulated rows: 2309888\n",
      "Processed chunk... current accumulated rows: 2318178\n",
      "Processed chunk... current accumulated rows: 2324669\n",
      "Processed chunk... current accumulated rows: 2333015\n",
      "Processed chunk... current accumulated rows: 2344237\n",
      "Processed chunk... current accumulated rows: 2354981\n",
      "Processed chunk... current accumulated rows: 2369004\n",
      "Processed chunk... current accumulated rows: 2377165\n",
      "Processed chunk... current accumulated rows: 2385746\n",
      "Processed chunk... current accumulated rows: 2393931\n",
      "Processed chunk... current accumulated rows: 2403843\n",
      "Processed chunk... current accumulated rows: 2414709\n",
      "Processed chunk... current accumulated rows: 2425997\n",
      "Processed chunk... current accumulated rows: 2435608\n",
      "Processed chunk... current accumulated rows: 2445455\n",
      "Processed chunk... current accumulated rows: 2455136\n",
      "Processed chunk... current accumulated rows: 2465131\n",
      "Processed chunk... current accumulated rows: 2473558\n",
      "Processed chunk... current accumulated rows: 2484260\n",
      "Processed chunk... current accumulated rows: 2493908\n",
      "Processed chunk... current accumulated rows: 2503885\n",
      "Processed chunk... current accumulated rows: 2514667\n",
      "Processed chunk... current accumulated rows: 2522975\n",
      "Processed chunk... current accumulated rows: 2533415\n",
      "Processed chunk... current accumulated rows: 2540959\n",
      "Processed chunk... current accumulated rows: 2549057\n",
      "Processed chunk... current accumulated rows: 2556919\n",
      "Processed chunk... current accumulated rows: 2566534\n",
      "Processed chunk... current accumulated rows: 2576080\n",
      "Processed chunk... current accumulated rows: 2583206\n",
      "Processed chunk... current accumulated rows: 2593436\n",
      "Processed chunk... current accumulated rows: 2607869\n",
      "Processed chunk... current accumulated rows: 2617271\n",
      "Processed chunk... current accumulated rows: 2624102\n",
      "Processed chunk... current accumulated rows: 2632302\n",
      "Processed chunk... current accumulated rows: 2639787\n",
      "Processed chunk... current accumulated rows: 2650367\n",
      "Processed chunk... current accumulated rows: 2657828\n",
      "Processed chunk... current accumulated rows: 2666309\n",
      "Processed chunk... current accumulated rows: 2677343\n",
      "Processed chunk... current accumulated rows: 2685314\n",
      "Processed chunk... current accumulated rows: 2693645\n",
      "Processed chunk... current accumulated rows: 2702388\n",
      "Processed chunk... current accumulated rows: 2712275\n",
      "Processed chunk... current accumulated rows: 2722463\n",
      "Processed chunk... current accumulated rows: 2731053\n",
      "Processed chunk... current accumulated rows: 2739061\n",
      "Processed chunk... current accumulated rows: 2748377\n",
      "Processed chunk... current accumulated rows: 2759127\n",
      "Processed chunk... current accumulated rows: 2765397\n",
      "Processed chunk... current accumulated rows: 2772360\n",
      "Processed chunk... current accumulated rows: 2778249\n",
      "Processed chunk... current accumulated rows: 2784728\n",
      "Processed chunk... current accumulated rows: 2795738\n",
      "Processed chunk... current accumulated rows: 2804005\n",
      "Processed chunk... current accumulated rows: 2812203\n",
      "Processed chunk... current accumulated rows: 2827010\n",
      "Processed chunk... current accumulated rows: 2839085\n",
      "Processed chunk... current accumulated rows: 2846135\n",
      "Processed chunk... current accumulated rows: 2854118\n",
      "Processed chunk... current accumulated rows: 2862920\n",
      "Processed chunk... current accumulated rows: 2871256\n",
      "Processed chunk... current accumulated rows: 2880891\n",
      "Processed chunk... current accumulated rows: 2894136\n",
      "Processed chunk... current accumulated rows: 2901134\n",
      "Processed chunk... current accumulated rows: 2906689\n",
      "Processed chunk... current accumulated rows: 2915216\n",
      "Processed chunk... current accumulated rows: 2928345\n",
      "Processed chunk... current accumulated rows: 2933487\n",
      "Processed chunk... current accumulated rows: 2942353\n",
      "Processed chunk... current accumulated rows: 2950405\n",
      "Processed chunk... current accumulated rows: 2956848\n",
      "Processed chunk... current accumulated rows: 2966402\n",
      "Processed chunk... current accumulated rows: 2972560\n",
      "Processed chunk... current accumulated rows: 2982999\n",
      "Processed chunk... current accumulated rows: 2993685\n",
      "Processed chunk... current accumulated rows: 3009028\n",
      "Processed chunk... current accumulated rows: 3019378\n",
      "Processed chunk... current accumulated rows: 3027856\n",
      "Processed chunk... current accumulated rows: 3035963\n",
      "Processed chunk... current accumulated rows: 3046016\n",
      "Processed chunk... current accumulated rows: 3054767\n",
      "Processed chunk... current accumulated rows: 3065193\n",
      "Processed chunk... current accumulated rows: 3075871\n",
      "Processed chunk... current accumulated rows: 3082950\n",
      "Processed chunk... current accumulated rows: 3091388\n",
      "Processed chunk... current accumulated rows: 3104103\n",
      "Processed chunk... current accumulated rows: 3115230\n",
      "Processed chunk... current accumulated rows: 3124700\n",
      "Processed chunk... current accumulated rows: 3133182\n",
      "Processed chunk... current accumulated rows: 3143446\n",
      "Processed chunk... current accumulated rows: 3151034\n",
      "Processed chunk... current accumulated rows: 3158210\n",
      "Processed chunk... current accumulated rows: 3166280\n",
      "Processed chunk... current accumulated rows: 3176077\n",
      "Processed chunk... current accumulated rows: 3190178\n",
      "Processed chunk... current accumulated rows: 3199370\n",
      "Processed chunk... current accumulated rows: 3208036\n",
      "Processed chunk... current accumulated rows: 3216341\n",
      "Processed chunk... current accumulated rows: 3223626\n",
      "Processed chunk... current accumulated rows: 3235617\n",
      "Processed chunk... current accumulated rows: 3245344\n",
      "Processed chunk... current accumulated rows: 3253089\n",
      "Processed chunk... current accumulated rows: 3264276\n",
      "Processed chunk... current accumulated rows: 3271922\n",
      "Processed chunk... current accumulated rows: 3280558\n",
      "Processed chunk... current accumulated rows: 3289418\n",
      "Processed chunk... current accumulated rows: 3298168\n",
      "Processed chunk... current accumulated rows: 3307891\n",
      "Processed chunk... current accumulated rows: 3314527\n",
      "Processed chunk... current accumulated rows: 3321221\n",
      "Processed chunk... current accumulated rows: 3328436\n",
      "Processed chunk... current accumulated rows: 3337089\n",
      "Processed chunk... current accumulated rows: 3344525\n",
      "Processed chunk... current accumulated rows: 3355049\n",
      "Processed chunk... current accumulated rows: 3362593\n",
      "Processed chunk... current accumulated rows: 3371106\n",
      "Processed chunk... current accumulated rows: 3384109\n",
      "Processed chunk... current accumulated rows: 3394042\n",
      "Processed chunk... current accumulated rows: 3404219\n",
      "Processed chunk... current accumulated rows: 3409205\n",
      "Processed chunk... current accumulated rows: 3418948\n",
      "Processed chunk... current accumulated rows: 3427676\n",
      "Processed chunk... current accumulated rows: 3434597\n",
      "Processed chunk... current accumulated rows: 3445700\n",
      "Processed chunk... current accumulated rows: 3452066\n",
      "Processed chunk... current accumulated rows: 3459727\n",
      "Processed chunk... current accumulated rows: 3467147\n",
      "Processed chunk... current accumulated rows: 3474985\n",
      "Processed chunk... current accumulated rows: 3485556\n",
      "Processed chunk... current accumulated rows: 3494486\n",
      "Processed chunk... current accumulated rows: 3508216\n",
      "Processed chunk... current accumulated rows: 3514383\n",
      "Processed chunk... current accumulated rows: 3520951\n",
      "Processed chunk... current accumulated rows: 3529428\n",
      "Processed chunk... current accumulated rows: 3538351\n",
      "Processed chunk... current accumulated rows: 3546679\n",
      "Processed chunk... current accumulated rows: 3555776\n",
      "Processed chunk... current accumulated rows: 3566731\n",
      "Processed chunk... current accumulated rows: 3576543\n",
      "Processed chunk... current accumulated rows: 3582082\n",
      "Processed chunk... current accumulated rows: 3592286\n",
      "Processed chunk... current accumulated rows: 3602017\n",
      "Processed chunk... current accumulated rows: 3611844\n",
      "Processed chunk... current accumulated rows: 3618976\n",
      "Processed chunk... current accumulated rows: 3630706\n",
      "Processed chunk... current accumulated rows: 3644809\n",
      "Processed chunk... current accumulated rows: 3654884\n",
      "Processed chunk... current accumulated rows: 3663701\n",
      "Processed chunk... current accumulated rows: 3672245\n",
      "Processed chunk... current accumulated rows: 3679764\n",
      "Processed chunk... current accumulated rows: 3689777\n",
      "Processed chunk... current accumulated rows: 3700386\n",
      "Processed chunk... current accumulated rows: 3710180\n",
      "Processed chunk... current accumulated rows: 3721869\n",
      "Processed chunk... current accumulated rows: 3729703\n",
      "Processed chunk... current accumulated rows: 3737928\n",
      "Processed chunk... current accumulated rows: 3747209\n",
      "Processed chunk... current accumulated rows: 3754505\n",
      "Processed chunk... current accumulated rows: 3761669\n",
      "Processed chunk... current accumulated rows: 3771764\n",
      "Processed chunk... current accumulated rows: 3780941\n",
      "Processed chunk... current accumulated rows: 3791108\n",
      "Processed chunk... current accumulated rows: 3801509\n",
      "Processed chunk... current accumulated rows: 3807650\n",
      "Processed chunk... current accumulated rows: 3817975\n",
      "Processed chunk... current accumulated rows: 3824334\n",
      "Processed chunk... current accumulated rows: 3831424\n",
      "Processed chunk... current accumulated rows: 3844232\n",
      "Processed chunk... current accumulated rows: 3853959\n",
      "Processed chunk... current accumulated rows: 3862487\n",
      "Processed chunk... current accumulated rows: 3869551\n",
      "Processed chunk... current accumulated rows: 3877016\n",
      "Processed chunk... current accumulated rows: 3885791\n",
      "Processed chunk... current accumulated rows: 3894088\n",
      "Processed chunk... current accumulated rows: 3899957\n",
      "Processed chunk... current accumulated rows: 3906056\n",
      "Processed chunk... current accumulated rows: 3916254\n",
      "Processed chunk... current accumulated rows: 3923235\n",
      "Processed chunk... current accumulated rows: 3929428\n",
      "Processed chunk... current accumulated rows: 3935307\n",
      "Processed chunk... current accumulated rows: 3943667\n",
      "Processed chunk... current accumulated rows: 3962063\n",
      "Processed chunk... current accumulated rows: 3970929\n",
      "Processed chunk... current accumulated rows: 3978964\n",
      "Processed chunk... current accumulated rows: 3985606\n",
      "Processed chunk... current accumulated rows: 3993706\n",
      "Processed chunk... current accumulated rows: 4005197\n",
      "Processed chunk... current accumulated rows: 4012661\n",
      "Processed chunk... current accumulated rows: 4022082\n",
      "Processed chunk... current accumulated rows: 4028771\n",
      "Processed chunk... current accumulated rows: 4038244\n",
      "Processed chunk... current accumulated rows: 4044694\n",
      "Processed chunk... current accumulated rows: 4057427\n",
      "Processed chunk... current accumulated rows: 4064185\n",
      "Processed chunk... current accumulated rows: 4071831\n",
      "Processed chunk... current accumulated rows: 4083253\n",
      "Processed chunk... current accumulated rows: 4093826\n",
      "Processed chunk... current accumulated rows: 4099452\n",
      "Processed chunk... current accumulated rows: 4107028\n",
      "Processed chunk... current accumulated rows: 4116432\n",
      "Processed chunk... current accumulated rows: 4125724\n",
      "Processed chunk... current accumulated rows: 4134527\n",
      "Processed chunk... current accumulated rows: 4147341\n",
      "Processed chunk... current accumulated rows: 4155101\n",
      "Processed chunk... current accumulated rows: 4168597\n",
      "Processed chunk... current accumulated rows: 4176088\n",
      "Processed chunk... current accumulated rows: 4187476\n",
      "Processed chunk... current accumulated rows: 4199986\n",
      "Processed chunk... current accumulated rows: 4214314\n",
      "Processed chunk... current accumulated rows: 4223018\n",
      "Processed chunk... current accumulated rows: 4230812\n",
      "Processed chunk... current accumulated rows: 4239837\n",
      "Processed chunk... current accumulated rows: 4249230\n",
      "Processed chunk... current accumulated rows: 4254818\n",
      "Processed chunk... current accumulated rows: 4263477\n",
      "Processed chunk... current accumulated rows: 4272365\n",
      "Processed chunk... current accumulated rows: 4281387\n",
      "Processed chunk... current accumulated rows: 4287045\n",
      "Processed chunk... current accumulated rows: 4293469\n",
      "Processed chunk... current accumulated rows: 4300316\n",
      "Processed chunk... current accumulated rows: 4311259\n",
      "Processed chunk... current accumulated rows: 4319276\n",
      "Processed chunk... current accumulated rows: 4328530\n",
      "Processed chunk... current accumulated rows: 4336227\n",
      "Processed chunk... current accumulated rows: 4343211\n",
      "Processed chunk... current accumulated rows: 4351794\n",
      "Processed chunk... current accumulated rows: 4362015\n",
      "Processed chunk... current accumulated rows: 4370972\n",
      "Processed chunk... current accumulated rows: 4379256\n",
      "Processed chunk... current accumulated rows: 4385544\n",
      "Processed chunk... current accumulated rows: 4398020\n",
      "Processed chunk... current accumulated rows: 4406500\n",
      "Processed chunk... current accumulated rows: 4415329\n",
      "Processed chunk... current accumulated rows: 4423453\n",
      "Processed chunk... current accumulated rows: 4434177\n",
      "Processed chunk... current accumulated rows: 4445313\n",
      "Processed chunk... current accumulated rows: 4455561\n",
      "Processed chunk... current accumulated rows: 4462913\n",
      "Processed chunk... current accumulated rows: 4471448\n",
      "Processed chunk... current accumulated rows: 4478397\n",
      "Processed chunk... current accumulated rows: 4484735\n",
      "Processed chunk... current accumulated rows: 4493872\n",
      "Processed chunk... current accumulated rows: 4503804\n",
      "Processed chunk... current accumulated rows: 4514045\n",
      "Processed chunk... current accumulated rows: 4522746\n",
      "Processed chunk... current accumulated rows: 4532579\n",
      "Processed chunk... current accumulated rows: 4539538\n",
      "Processed chunk... current accumulated rows: 4546649\n",
      "Processed chunk... current accumulated rows: 4556207\n",
      "Processed chunk... current accumulated rows: 4568479\n",
      "Processed chunk... current accumulated rows: 4576252\n",
      "Processed chunk... current accumulated rows: 4589690\n",
      "Processed chunk... current accumulated rows: 4597655\n",
      "Processed chunk... current accumulated rows: 4607758\n",
      "Processed chunk... current accumulated rows: 4614893\n",
      "Processed chunk... current accumulated rows: 4624781\n",
      "Processed chunk... current accumulated rows: 4631224\n",
      "Processed chunk... current accumulated rows: 4640202\n",
      "Processed chunk... current accumulated rows: 4649939\n",
      "Processed chunk... current accumulated rows: 4661782\n",
      "Processed chunk... current accumulated rows: 4670133\n",
      "Processed chunk... current accumulated rows: 4678642\n",
      "Processed chunk... current accumulated rows: 4685166\n",
      "Processed chunk... current accumulated rows: 4692339\n",
      "Processed chunk... current accumulated rows: 4703298\n",
      "Processed chunk... current accumulated rows: 4710737\n",
      "Processed chunk... current accumulated rows: 4719246\n",
      "Processed chunk... current accumulated rows: 4732256\n",
      "Processed chunk... current accumulated rows: 4742381\n",
      "Processed chunk... current accumulated rows: 4752846\n",
      "Processed chunk... current accumulated rows: 4763156\n",
      "Processed chunk... current accumulated rows: 4768391\n",
      "Processed chunk... current accumulated rows: 4776135\n",
      "Processed chunk... current accumulated rows: 4783987\n",
      "Processed chunk... current accumulated rows: 4793366\n",
      "Processed chunk... current accumulated rows: 4804266\n",
      "Processed chunk... current accumulated rows: 4818115\n",
      "Processed chunk... current accumulated rows: 4831794\n",
      "Processed chunk... current accumulated rows: 4842057\n",
      "Processed chunk... current accumulated rows: 4853230\n",
      "Processed chunk... current accumulated rows: 4861370\n",
      "Processed chunk... current accumulated rows: 4870651\n",
      "Processed chunk... current accumulated rows: 4878931\n",
      "Processed chunk... current accumulated rows: 4886921\n",
      "Processed chunk... current accumulated rows: 4894705\n",
      "Processed chunk... current accumulated rows: 4900969\n",
      "Processed chunk... current accumulated rows: 4913693\n",
      "Processed chunk... current accumulated rows: 4921491\n",
      "Processed chunk... current accumulated rows: 4930353\n",
      "Processed chunk... current accumulated rows: 4940376\n",
      "Processed chunk... current accumulated rows: 4946781\n",
      "Processed chunk... current accumulated rows: 4954781\n",
      "Processed chunk... current accumulated rows: 4961665\n",
      "Processed chunk... current accumulated rows: 4973001\n",
      "Processed chunk... current accumulated rows: 4981160\n",
      "Processed chunk... current accumulated rows: 4994490\n",
      "Processed chunk... current accumulated rows: 5006743\n",
      "Processed chunk... current accumulated rows: 5015894\n",
      "Processed chunk... current accumulated rows: 5027788\n",
      "Processed chunk... current accumulated rows: 5033270\n",
      "Processed chunk... current accumulated rows: 5044824\n",
      "Processed chunk... current accumulated rows: 5053162\n",
      "Processed chunk... current accumulated rows: 5061582\n",
      "Processed chunk... current accumulated rows: 5072219\n",
      "Processed chunk... current accumulated rows: 5079345\n",
      "Processed chunk... current accumulated rows: 5087109\n",
      "Processed chunk... current accumulated rows: 5098526\n",
      "Processed chunk... current accumulated rows: 5105282\n",
      "Processed chunk... current accumulated rows: 5111447\n",
      "Processed chunk... current accumulated rows: 5122322\n",
      "Processed chunk... current accumulated rows: 5130888\n",
      "Processed chunk... current accumulated rows: 5140817\n",
      "Processed chunk... current accumulated rows: 5149226\n",
      "Processed chunk... current accumulated rows: 5158747\n",
      "Processed chunk... current accumulated rows: 5170189\n",
      "Processed chunk... current accumulated rows: 5178593\n",
      "Processed chunk... current accumulated rows: 5187364\n",
      "Processed chunk... current accumulated rows: 5195626\n",
      "Processed chunk... current accumulated rows: 5201229\n",
      "Processed chunk... current accumulated rows: 5210370\n",
      "Processed chunk... current accumulated rows: 5217696\n",
      "Processed chunk... current accumulated rows: 5226425\n",
      "Processed chunk... current accumulated rows: 5234451\n",
      "Processed chunk... current accumulated rows: 5243718\n",
      "Processed chunk... current accumulated rows: 5255251\n",
      "Processed chunk... current accumulated rows: 5265733\n",
      "Processed chunk... current accumulated rows: 5276411\n",
      "Processed chunk... current accumulated rows: 5288373\n",
      "Processed chunk... current accumulated rows: 5294046\n",
      "Processed chunk... current accumulated rows: 5303817\n",
      "Processed chunk... current accumulated rows: 5312867\n",
      "Processed chunk... current accumulated rows: 5324829\n",
      "Processed chunk... current accumulated rows: 5337506\n",
      "Processed chunk... current accumulated rows: 5346785\n",
      "Processed chunk... current accumulated rows: 5356254\n",
      "Processed chunk... current accumulated rows: 5362128\n",
      "Processed chunk... current accumulated rows: 5370594\n",
      "Processed chunk... current accumulated rows: 5380903\n",
      "Processed chunk... current accumulated rows: 5393323\n",
      "Processed chunk... current accumulated rows: 5402912\n",
      "Processed chunk... current accumulated rows: 5410749\n",
      "Processed chunk... current accumulated rows: 5418361\n",
      "Processed chunk... current accumulated rows: 5423512\n",
      "Processed chunk... current accumulated rows: 5432223\n",
      "Processed chunk... current accumulated rows: 5442454\n",
      "Processed chunk... current accumulated rows: 5450322\n",
      "Processed chunk... current accumulated rows: 5459352\n",
      "Processed chunk... current accumulated rows: 5469859\n",
      "Final Sample Size: 5469859 rows\n",
      "Saved to /Users/duc.tran/mobile-item-recommendation/data/items.parquet\n"
     ]
    }
   ],
   "source": [
    "def create_sample_dataset(\n",
    "    RAW_ITEM_FILE=RAW_ITEM_DATA_PATH, \n",
    "    RAW_USER_FILE=RAW_USER_DATA_PATH):\n",
    "\n",
    "    print(f\"Reading raw data (Sample Rate: {SAMPLE_RATE*100}%)...\")\n",
    "    \n",
    "    # 1. Load the Item Subset first (We strictly need these items)\n",
    "    df_item = pd.read_csv(\n",
    "        RAW_ITEM_FILE, \n",
    "        header=None, \n",
    "        names=['item_id', 'item_geohash', 'item_category'],\n",
    "        sep='\\t',\n",
    "        dtype={'item_id': 'int32', 'item_category': 'int32'}\n",
    "    )\n",
    "    valid_items = set(df_item['item_id'].unique())\n",
    "    print(f\"Item Subset Size: {len(valid_items)}\")\n",
    "\n",
    "    # 2. Read User Data in Chunks\n",
    "    chunksize = 1_000_000\n",
    "    chunks = []\n",
    "    \n",
    "    # We will select a random set of users to keep\n",
    "    # But first, we need to know which users exist. \n",
    "    # Since we can't read all users easily, we filter on the fly.\n",
    "    # A hash-based sampling is deterministic and efficient.\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        RAW_USER_FILE, \n",
    "        header=None,\n",
    "        names=['user_id', 'item_id', 'behavior_type', 'user_geohash', 'item_category', 'time'],\n",
    "        sep='\\t',\n",
    "        dtype={'user_id': 'int32', 'item_id': 'int32', 'behavior_type': 'int8', 'item_category': 'int32'},\n",
    "        chunksize=chunksize\n",
    "    ):\n",
    "        # Filter 1: Keep only interactions with valid items\n",
    "        chunk = chunk[chunk['item_id'].isin(valid_items)]\n",
    "        \n",
    "        # Filter 2: Hash-based sampling\n",
    "        # We keep user if user_id % 100 < (SAMPLE_RATE * 100)\n",
    "        # This keeps exactly 10% of users, and it keeps ALL their records.\n",
    "        chunk = chunk[chunk['user_id'] % 100 < (SAMPLE_RATE * 100)]\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "        print(f\"Processed chunk... current accumulated rows: {sum(len(c) for c in chunks)}\")\n",
    "\n",
    "    # 3. Concatenate and Save\n",
    "    full_sample = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Parse dates only at the end to save processing time\n",
    "    full_sample['time'] = pd.to_datetime(full_sample['time'])\n",
    "    full_sample['date'] = full_sample['time'].dt.date\n",
    "    \n",
    "    print(f\"Final Sample Size: {len(full_sample)} rows\")\n",
    "    full_sample.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(f\"Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "create_sample_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cad6a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting raw text files...\n",
      "Data ingested and saved to Parquet. Items: 6781009\n",
      "Loading from Parquet (Fast)...\n"
     ]
    }
   ],
   "source": [
    "def ingest_data(RAW_ITEM_FILE):\n",
    "    print(\"Ingesting raw text files...\")\n",
    "    \n",
    "    df_item = pd.read_csv(\n",
    "        RAW_ITEM_FILE, \n",
    "        header=None, \n",
    "        names=['item_id', 'item_geohash', 'item_category'],\n",
    "        sep='\\t',\n",
    "        dtype={'item_id': 'int32', 'item_category': 'int32'}\n",
    "    )\n",
    "\n",
    "    # Save as Parquet (The \"Silver\" Layer)\n",
    "    df_item.to_parquet(f\"{DATA_PATH}/items.parquet\", index=False)\n",
    "    print(f\"Data ingested and saved to Parquet. Items: {len(df_item)}\")\n",
    "\n",
    "if not os.path.exists(f\"{DATA_PATH}/items.parquet\"):\n",
    "    ingest_data(RAW_ITEM_DATA_PATH)\n",
    "\n",
    "def load_parquet_data():\n",
    "    print(\"Loading from Parquet (Fast)...\")\n",
    "    df_item = pd.read_parquet(f\"{DATA_PATH}/items.parquet\")\n",
    "    return df_item\n",
    "\n",
    "df_item = load_parquet_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9cfc0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_USERS = f\"{DATA_PATH}/users.parquet\"\n",
    "RAW_ITEMS = f\"{DATA_PATH}/items.parquet\"\n",
    "\n",
    "TRAIN_OUT = f\"{DATA_PATH}/train_processed.parquet\"\n",
    "VAL_OUT = f\"{DATA_PATH}/val_processed.parquet\"\n",
    "TEST_OUT = f\"{DATA_PATH}/test_processed.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "278e7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_features(df, target_date_str, window_days=[1, 3]):\n",
    "\n",
    "    target_ts = pd.Timestamp(target_date_str)\n",
    "    target_date = target_ts.date()\n",
    "    logger.info(f\"\\n--- Processing Target Date: {target_date} ---\")\n",
    "    \n",
    "    # 1. Define History (Features) vs Target (Labels)\n",
    "    history_df = df[df['date'] < target_date]\n",
    "    target_df = df[df['date'] == target_date]\n",
    "    \n",
    "    # 2. Candidate Generation (The \"Anchor\")\n",
    "    start_anchor = target_date - timedelta(days=1)\n",
    "    candidates = history_df[history_df['date'] >= start_anchor][['user_id', 'item_id', 'item_category']].drop_duplicates()\n",
    "    \n",
    "    # 3. Helper: Calculate Statistics for a Window\n",
    "    def get_window_stats(subset_df, prefix):\n",
    "        # --- A. Item Features (Popularity) ---\n",
    "        item_stats = subset_df.groupby(['item_id', 'behavior_type']).size().unstack(fill_value=0)\n",
    "        # Ensure cols 1-4 exist\n",
    "        for i in [1, 2, 3, 4]: \n",
    "            if i not in item_stats: item_stats[i] = 0\n",
    "        item_stats.columns = [f'{prefix}_item_clk', f'{prefix}_item_fav', f'{prefix}_item_cart', f'{prefix}_item_buy']\n",
    "        \n",
    "        # Item Conversion Rate (Buy/Click)\n",
    "        item_stats[f'{prefix}_item_cr'] = item_stats[f'{prefix}_item_buy'] / (item_stats[f'{prefix}_item_clk'] + 1)\n",
    "        \n",
    "        # Unique Users per Item\n",
    "        item_uniq = subset_df.groupby('item_id')['user_id'].nunique().to_frame(f'{prefix}_item_uniq_users')\n",
    "        item_stats = item_stats.join(item_uniq)\n",
    "\n",
    "        # --- B. User Features (Activity) ---\n",
    "        user_stats = subset_df.groupby(['user_id', 'behavior_type']).size().unstack(fill_value=0)\n",
    "        for i in [1, 2, 3, 4]: \n",
    "            if i not in user_stats: user_stats[i] = 0\n",
    "        user_stats.columns = [f'{prefix}_user_clk', f'{prefix}_user_fav', f'{prefix}_user_cart', f'{prefix}_user_buy']\n",
    "        \n",
    "        # User Conversion Rate\n",
    "        user_stats[f'{prefix}_user_cr'] = user_stats[f'{prefix}_user_buy'] / (user_stats[f'{prefix}_user_clk'] + 1)\n",
    "\n",
    "        # --- C. User-Item Interaction (Specific Interest) ---\n",
    "        ui_stats = subset_df.groupby(['user_id', 'item_id', 'behavior_type']).size().unstack(fill_value=0)\n",
    "        for i in [1, 2, 3, 4]: \n",
    "            if i not in ui_stats: ui_stats[i] = 0\n",
    "        ui_stats.columns = [f'{prefix}_ui_clk', f'{prefix}_ui_fav', f'{prefix}_ui_cart', f'{prefix}_ui_buy']\n",
    "\n",
    "        # --- D. User-Category Interaction (Broader Interest - Adapted from baseline) ---\n",
    "        uc_stats = subset_df.groupby(['user_id', 'item_category', 'behavior_type']).size().unstack(fill_value=0)\n",
    "        for i in [1, 2, 3, 4]: \n",
    "            if i not in uc_stats: uc_stats[i] = 0\n",
    "        uc_stats.columns = [f'{prefix}_uc_clk', f'{prefix}_uc_fav', f'{prefix}_uc_cart', f'{prefix}_uc_buy']\n",
    "\n",
    "        return item_stats, user_stats, ui_stats, uc_stats\n",
    "\n",
    "    # 4. Sliding Window Loop (Last 1 day, Last 3 days)\n",
    "    for days in window_days:\n",
    "        start_date = target_date - timedelta(days=days)\n",
    "        window_df = history_df[history_df['date'] >= start_date]\n",
    "        \n",
    "        prefix = f\"last_{days}d\"\n",
    "        i_stats, u_stats, ui_stats, uc_stats = get_window_stats(window_df, prefix)\n",
    "        \n",
    "        # Merge sequentially\n",
    "        # Note: We merge on different keys for different feature sets\n",
    "        candidates = candidates.merge(i_stats, on='item_id', how='left').fillna(0)\n",
    "        candidates = candidates.merge(u_stats, on='user_id', how='left').fillna(0)\n",
    "        candidates = candidates.merge(ui_stats, on=['user_id', 'item_id'], how='left').fillna(0)\n",
    "        candidates = candidates.merge(uc_stats, on=['user_id', 'item_category'], how='left').fillna(0)\n",
    "\n",
    "    # 5. Time & Recency Features\n",
    "    # Get the hour of the last interaction\n",
    "    last_interaction = history_df.sort_values('time').groupby(['user_id', 'item_id']).tail(1)\n",
    "    last_interaction['last_touch_hour'] = last_interaction['time'].dt.hour\n",
    "    last_interaction['date'] = pd.to_datetime(last_interaction['date'])\n",
    "    last_interaction['days_since'] = (target_ts - last_interaction['date']).dt.days\n",
    "    \n",
    "    candidates = candidates.merge(\n",
    "        last_interaction[['user_id', 'item_id', 'last_touch_hour', 'days_since']], \n",
    "        on=['user_id', 'item_id'], \n",
    "        how='left'\n",
    "    ).fillna(-1) \n",
    "\n",
    "    # 6. LABEL GENERATION (The Ground Truth)\n",
    "    # We check if the user bought the item ON the target date\n",
    "    buys_on_target = target_df[target_df['behavior_type'] == 4]\n",
    "    \n",
    "    # Create a set of (user, item) tuples that were bought\n",
    "    positive_pairs = set(zip(buys_on_target['user_id'], buys_on_target['item_id']))\n",
    "    \n",
    "    # Apply Label: 1 if bought, 0 otherwise\n",
    "    candidates['label'] = candidates.apply(\n",
    "        lambda x: 1 if (x['user_id'], x['item_id']) in positive_pairs else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Candidates: {len(candidates)} | Positives (Label=1): {candidates['label'].sum()}\")\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "89090454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = pd.read_parquet(RAW_USERS)\n",
    "df_item = pd.read_parquet(RAW_ITEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2e03a980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>behavior_type</th>\n",
       "      <th>user_geohash</th>\n",
       "      <th>item_category</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77413503</td>\n",
       "      <td>159477064</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 13:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77413503</td>\n",
       "      <td>392053776</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 20:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77413503</td>\n",
       "      <td>80129884</td>\n",
       "      <td>1</td>\n",
       "      <td>9rmq1tj</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 11:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77413503</td>\n",
       "      <td>360889319</td>\n",
       "      <td>1</td>\n",
       "      <td>9rmq6fb</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 10:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77413503</td>\n",
       "      <td>80129884</td>\n",
       "      <td>1</td>\n",
       "      <td>9rmq1tk</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 11:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id    item_id  behavior_type user_geohash  item_category  \\\n",
       "0  77413503  159477064              1         None           3846   \n",
       "1  77413503  392053776              1         None           3846   \n",
       "2  77413503   80129884              1      9rmq1tj           3846   \n",
       "3  77413503  360889319              1      9rmq6fb           3846   \n",
       "4  77413503   80129884              1      9rmq1tk           3846   \n",
       "\n",
       "                 time        date  \n",
       "0 2014-12-13 13:00:00  2014-12-13  \n",
       "1 2014-12-13 20:00:00  2014-12-13  \n",
       "2 2014-12-13 11:00:00  2014-12-13  \n",
       "3 2014-12-13 10:00:00  2014-12-13  \n",
       "4 2014-12-13 11:00:00  2014-12-13  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "470adcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-27 10:17:23.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_train_val_test_splits\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mLoading data...\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:28.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_train_val_test_splits\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mGenerating Training set...\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:28.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_advanced_features\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1m\n",
      "--- Processing Target Date: 2014-12-08 ---\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:30.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_advanced_features\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mCandidates: 81266 | Positives (Label=1): 245\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:30.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_train_val_test_splits\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mGenerating Validation set...\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:30.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_advanced_features\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1m\n",
      "--- Processing Target Date: 2014-12-12 ---\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:35.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_advanced_features\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mCandidates: 88911 | Positives (Label=1): 1756\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:35.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_train_val_test_splits\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mGenerating Test set...\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:35.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_advanced_features\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1m\n",
      "--- Processing Target Date: 2014-12-18 ---\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:39.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_advanced_features\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mCandidates: 75927 | Positives (Label=1): 261\u001b[0m\n",
      "\u001b[32m2025-11-27 10:17:39.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_train_val_test_splits\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mAll sets generated successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def prepare_train_val_test_splits(RAW_USER_FILE, RAW_ITEM_FILE):\n",
    "\n",
    "    logger.info(\"Loading data...\")\n",
    "    df_user = pd.read_parquet(RAW_USER_FILE)\n",
    "    df_item = pd.read_parquet(RAW_ITEM_FILE)\n",
    "\n",
    "    valid_items = set(df_item['item_id'])\n",
    "    df_user = df_user[df_user['item_id'].isin(valid_items)]\n",
    "    df_user['date'] = df_user['time'].dt.date\n",
    "\n",
    "    logger.info(\"Generating Training set...\")\n",
    "    train_df = extract_advanced_features(df_user, \"2014-12-08\")\n",
    "    train_df.to_parquet(TRAIN_OUT, index=False)\n",
    "\n",
    "    logger.info(\"Generating Validation set...\")\n",
    "    val_df = extract_advanced_features(df_user, \"2014-12-12\")\n",
    "    val_df.to_parquet(VAL_OUT, index=False)\n",
    "    \n",
    "    logger.info(\"Generating Test set...\")\n",
    "    test_df = extract_advanced_features(df_user, \"2014-12-18\")\n",
    "    test_df.to_parquet(TEST_OUT, index=False)\n",
    "\n",
    "    logger.info(\"All sets generated successfully!\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = prepare_train_val_test_splits(RAW_USERS, RAW_ITEMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7425691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = f\"{DATA_PATH}/train_processed.parquet\"\n",
    "VAL_PATH = f\"{DATA_PATH}/val_processed.parquet\"\n",
    "TEST_PATH = f\"{DATA_PATH}/test_processed.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "52f39266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>behavior_type</th>\n",
       "      <th>user_geohash</th>\n",
       "      <th>item_category</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77413503</td>\n",
       "      <td>159477064</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 13:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77413503</td>\n",
       "      <td>392053776</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 20:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77413503</td>\n",
       "      <td>80129884</td>\n",
       "      <td>1</td>\n",
       "      <td>9rmq1tj</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 11:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77413503</td>\n",
       "      <td>360889319</td>\n",
       "      <td>1</td>\n",
       "      <td>9rmq6fb</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 10:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77413503</td>\n",
       "      <td>80129884</td>\n",
       "      <td>1</td>\n",
       "      <td>9rmq1tk</td>\n",
       "      <td>3846</td>\n",
       "      <td>2014-12-13 11:00:00</td>\n",
       "      <td>2014-12-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id    item_id  behavior_type user_geohash  item_category  \\\n",
       "0  77413503  159477064              1         None           3846   \n",
       "1  77413503  392053776              1         None           3846   \n",
       "2  77413503   80129884              1      9rmq1tj           3846   \n",
       "3  77413503  360889319              1      9rmq6fb           3846   \n",
       "4  77413503   80129884              1      9rmq1tk           3846   \n",
       "\n",
       "                 time        date  \n",
       "0 2014-12-13 13:00:00  2014-12-13  \n",
       "1 2014-12-13 20:00:00  2014-12-13  \n",
       "2 2014-12-13 11:00:00  2014-12-13  \n",
       "3 2014-12-13 10:00:00  2014-12-13  \n",
       "4 2014-12-13 11:00:00  2014-12-13  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82013ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, ratio=20):\n",
    "    positives = df[df['label'] == 1]\n",
    "    negatives = df[df['label'] == 0]\n",
    "    \n",
    "    if len(negatives) > len(positives) * ratio:\n",
    "        negatives = negatives.sample(n=len(positives) * ratio, random_state=42)\n",
    "    \n",
    "    return pd.concat([positives, negatives]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "922bff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, dataset_df, feature_cols, total_daily_buys=None, dataset_name=\"Validation\"):\n",
    "\n",
    "    preds = model.predict(dataset_df[feature_cols])\n",
    "\n",
    "    pred_indices = (preds == 1)\n",
    "    prediction_pairs = set(zip(dataset_df.loc[pred_indices, 'user_id'], dataset_df.loc[pred_indices, 'item_id']))\n",
    "\n",
    "    # Actual Positives IN CANDIDATE SET: (User, Item) where label was 1\n",
    "    actual_indices = (dataset_df['label'] == 1)\n",
    "    reference_pairs = set(zip(dataset_df.loc[actual_indices, 'user_id'], dataset_df.loc[actual_indices, 'item_id']))\n",
    "\n",
    "    # Intersection = True Positives found by the model\n",
    "    intersection = prediction_pairs & reference_pairs\n",
    "    tp_count = len(intersection)\n",
    "\n",
    "    # Precision is always (TP / Predicted)\n",
    "    precision = tp_count / len(prediction_pairs) if prediction_pairs else 0.0\n",
    "\n",
    "    # Recall Logic\n",
    "    if total_daily_buys:\n",
    "        # GLOBAL RECALL: TP / All purchases in the world that day\n",
    "        recall = tp_count / total_daily_buys\n",
    "        metric_type = \"Global\"\n",
    "    else:\n",
    "        # LOCAL RECALL: TP / All purchases in our candidate set\n",
    "        recall = tp_count / len(reference_pairs) if reference_pairs else 0.0\n",
    "        metric_type = \"Local\"\n",
    "\n",
    "    # F1 Calculation\n",
    "    f1 = 0.0 if (precision + recall) == 0 else (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    logger.info(f\"[{dataset_name} - {metric_type}] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "feaf3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"class_weight\": 'balanced',\n",
    "        \"max_iter\": 2000,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": 50,\n",
    "        \"max_depth\": 10,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": 10,\n",
    "        \"max_depth\": 3,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"eval_metric\": 'logloss',\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "}\n",
    "def train_and_evaluate(model_params=model_params, mlflow_experiment=MLFLOW_EXPERIMENT_NAME):\n",
    "    \n",
    "    logger.info(f\"Loading data from {DATA_PATH}...\")\n",
    "    train_df = pd.read_parquet(TRAIN_PATH)\n",
    "    val_df = pd.read_parquet(VAL_PATH)\n",
    "    test_df = pd.read_parquet(TEST_PATH)\n",
    "    \n",
    "    raw_df = pd.read_parquet(f\"{DATA_PATH}/users.parquet\")\n",
    "    val_date = pd.Timestamp(\"2014-12-12\").date()\n",
    "    val_global_buys = len(raw_df[(raw_df['date'] == val_date) & (raw_df['behavior_type'] == 4)])\n",
    "\n",
    "    test_date = pd.Timestamp(\"2014-12-18\").date()\n",
    "    test_global_buys = len(raw_df[(raw_df['date'] == test_date) & (raw_df['behavior_type'] == 4)])\n",
    "    \n",
    "    logger.info(f\"Global Buys - Val: {val_global_buys}, Test: {test_global_buys}\")\n",
    "    \n",
    "    # Separate Features and Target\n",
    "    drop_cols = ['user_id', 'item_id', 'item_category', 'label']\n",
    "    feature_cols = [c for c in train_df.columns if c not in drop_cols]\n",
    "    \n",
    "    # DOWNSAMPLING (Train set only)\n",
    "    logger.info(\"Downsampling negative class in training set (1:20 ratio)...\")\n",
    "    train_balanced = downsample(train_df, ratio=20)\n",
    "    X_train_bal = train_balanced[feature_cols]\n",
    "    y_train_bal = train_balanced['label']\n",
    "\n",
    "    signature = infer_signature(X_train_bal, y_train_bal)\n",
    "    input_example = X_train_bal.head(1)\n",
    "\n",
    "    mlflow.set_experiment(mlflow_experiment)\n",
    "    best_model_info = {\"f1\": -1, \"run_id\": None, \"description\": None}\n",
    "    \n",
    "    def run_model(model_name, model_obj, params):\n",
    "        nonlocal best_model_info\n",
    "        with mlflow.start_run(run_name=model_name):\n",
    "            logger.info(f\"Training {model_name}...\")\n",
    "            model_obj.fit(X_train_bal, y_train_bal)\n",
    "            \n",
    "            # 1. Local Evaluation (Candidate Set)\n",
    "            p_local, r_local, f1_local = evaluate_model(model_obj, val_df, feature_cols, total_daily_buys=None, dataset_name=\"Val_Local\")\n",
    "            \n",
    "            # 2. Global Evaluation (Business Metric)\n",
    "            p_global, r_global, f1_global = evaluate_model(model_obj, val_df, feature_cols, total_daily_buys=val_global_buys, dataset_name=\"Val_Global\")\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"val_f1_local\": f1_local,\n",
    "                \"val_precision_local\": p_local,\n",
    "                \"val_recall_local\": r_local,\n",
    "                \"val_f1_global\": f1_global, \n",
    "                \"val_recall_global\": r_global\n",
    "            })\n",
    "            mlflow.log_params({\n",
    "                \"model_type\": model_name.split('_')[1], # e.g. \"LogisticRegression\"\n",
    "                **params\n",
    "            })\n",
    "            mlflow.sklearn.log_model(model_obj, name=\"model\", signature=signature, input_example=input_example)\n",
    "\n",
    "            # Use Local F1 for model selection (since Global F1 is just a scaled version of Local F1 for the same candidate set)\n",
    "            if f1_local > best_model_info[\"f1\"]:\n",
    "                active_run = mlflow.active_run()\n",
    "                best_model_info = {\n",
    "                    \"f1\": f1_local,\n",
    "                    \"run_id\": active_run.info.run_id if active_run else None,\n",
    "                    \"description\": model_name\n",
    "                }\n",
    "\n",
    "    # Run Baselines\n",
    "    run_model(\"baseline_logistic_regression\", LogisticRegression(**model_params[\"LogisticRegression\"]), model_params[\"LogisticRegression\"])\n",
    "    run_model(\"baseline_random_forest\", RandomForestClassifier(**model_params[\"RandomForest\"]), model_params[\"RandomForest\"])\n",
    "    run_model(\"baseline_xgboost\", xgb.XGBClassifier(**model_params[\"XGBoost\"]), model_params[\"XGBoost\"])\n",
    "\n",
    "    def optuna_objective(trial):\n",
    "        tuned_params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1e-1, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**tuned_params)\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "        preds = model.predict(val_df[feature_cols])\n",
    "        return f1_score(val_df['label'], preds)\n",
    "\n",
    "    logger.info(\"Running Optuna tuning for XGBoost...\")\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(optuna_objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "    best_params = {\n",
    "        **study.best_params,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"xgboost_optuna_tuned\"):\n",
    "        tuned_model = xgb.XGBClassifier(**best_params)\n",
    "        tuned_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "        # Validation Metrics (Local & Global)\n",
    "        p_val_local, r_val_local, f1_val_local = evaluate_model(tuned_model, val_df, feature_cols, total_daily_buys=None, dataset_name=\"Val_Local\")\n",
    "        p_val_global, r_val_global, f1_val_global = evaluate_model(tuned_model, val_df, feature_cols, total_daily_buys=val_global_buys, dataset_name=\"Val_Global\")\n",
    "        \n",
    "        # Test Metrics (Local & Global) - This is your FINAL reported score\n",
    "        p_test_local, r_test_local, f1_test_local = evaluate_model(tuned_model, test_df, feature_cols, total_daily_buys=None, dataset_name=\"Test_Local\")\n",
    "        p_test_global, r_test_global, f1_test_global = evaluate_model(tuned_model, test_df, feature_cols, total_daily_buys=test_global_buys, dataset_name=\"Test_Global\")\n",
    "\n",
    "        mlflow.log_params({\n",
    "            \"model_type\": \"XGBoost_Optuna\",\n",
    "            **best_params\n",
    "        })\n",
    "        mlflow.log_metrics({\n",
    "            \"val_f1_local\": f1_val_local,\n",
    "            \"val_f1_global\": f1_val_global,\n",
    "            \"test_f1_local\": f1_test_local,\n",
    "            \"test_f1_global\": f1_test_global,\n",
    "            \"best_trial\": study.best_trial.number\n",
    "        })\n",
    "        mlflow.sklearn.log_model(tuned_model, name=\"model\", signature=signature, input_example=input_example)\n",
    "\n",
    "        if f1_val_local > best_model_info[\"f1\"]:\n",
    "            active_run = mlflow.active_run()\n",
    "            best_model_info = {\n",
    "                \"f1\": f1_val_local,\n",
    "                \"run_id\": active_run.info.run_id if active_run else None,\n",
    "                \"description\": \"xgboost_optuna_tuned\"\n",
    "            }\n",
    "\n",
    "    if best_model_info[\"run_id\"]:\n",
    "        logger.info(f\"Registering best model ({best_model_info['description']}) with val F1={best_model_info['f1']:.4f}\")\n",
    "        mlflow.register_model(f\"runs:/{best_model_info['run_id']}/model\", \"mobile_item_rec_model\")\n",
    "    else:\n",
    "        logger.warning(\"No model run recorded for registration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4b67ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-27 10:45:44.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_and_evaluate\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mLoading data from /Users/duc.tran/mobile-item-recommendation/data/raw...\u001b[0m\n",
      "\u001b[32m2025-11-27 10:45:44.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_and_evaluate\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mGlobal Buys - Val: 7415, Test: 1952\u001b[0m\n",
      "\u001b[32m2025-11-27 10:45:44.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_and_evaluate\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mDownsampling negative class in training set (1:20 ratio)...\u001b[0m\n",
      "/Users/duc.tran/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-11-27 10:45:45.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_model\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mTraining baseline_logistic_regression...\u001b[0m\n",
      "/Users/duc.tran/mobile-item-recommendation/venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m2025-11-27 10:45:45.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m[Val_Local - Local] Precision: 0.0914, Recall: 0.6617, F1: 0.1606\u001b[0m\n",
      "\u001b[32m2025-11-27 10:45:45.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_model\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m[Val_Global - Global] Precision: 0.0914, Recall: 0.1567, F1: 0.1155\u001b[0m\n",
      "Python(51250) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run baseline_logistic_regression at: http://localhost:5000/#/experiments/1/runs/2fa9a13bdfa44e6a98bf4c32aeb90ed4\n",
      " View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/app'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow_experiment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMLFLOW_EXPERIMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model_params, mlflow_experiment)\u001b[39m\n\u001b[32m     80\u001b[39m             best_model_info = {\n\u001b[32m     81\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: f1_local,\n\u001b[32m     82\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m: active_run.info.run_id \u001b[38;5;28;01mif\u001b[39;00m active_run \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     83\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m: model_name\n\u001b[32m     84\u001b[39m             }\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Run Baselines\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbaseline_logistic_regression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLogisticRegression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLogisticRegression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m run_model(\u001b[33m\"\u001b[39m\u001b[33mbaseline_random_forest\u001b[39m\u001b[33m\"\u001b[39m, RandomForestClassifier(**model_params[\u001b[33m\"\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m\"\u001b[39m]), model_params[\u001b[33m\"\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     89\u001b[39m run_model(\u001b[33m\"\u001b[39m\u001b[33mbaseline_xgboost\u001b[39m\u001b[33m\"\u001b[39m, xgb.XGBClassifier(**model_params[\u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m]), model_params[\u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mtrain_and_evaluate.<locals>.run_model\u001b[39m\u001b[34m(model_name, model_obj, params)\u001b[39m\n\u001b[32m     64\u001b[39m mlflow.log_metrics({\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_f1_local\u001b[39m\u001b[33m\"\u001b[39m: f1_local,\n\u001b[32m     66\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_precision_local\u001b[39m\u001b[33m\"\u001b[39m: p_local,\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_recall_global\u001b[39m\u001b[33m\"\u001b[39m: r_global\n\u001b[32m     70\u001b[39m })\n\u001b[32m     71\u001b[39m mlflow.log_params({\n\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m: model_name.split(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m1\u001b[39m], \u001b[38;5;66;03m# e.g. \"LogisticRegression\"\u001b[39;00m\n\u001b[32m     73\u001b[39m     **params\n\u001b[32m     74\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43msklearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Use Local F1 for model selection (since Global F1 is just a scaled version of Local F1 for the same candidate set)\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f1_local > best_model_info[\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/sklearn/__init__.py:426\u001b[39m, in \u001b[36mlog_model\u001b[39m\u001b[34m(sk_model, artifact_path, conda_env, code_paths, serialization_format, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, pyfunc_predict_fn, metadata, params, tags, model_type, step, model_id, name)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;129m@format_docstring\u001b[39m(LOG_MODEL_PARAM_DOCS.format(package_name=\u001b[33m\"\u001b[39m\u001b[33mscikit-learn\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_model\u001b[39m(\n\u001b[32m    336\u001b[39m     sk_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    355\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    356\u001b[39m ):\n\u001b[32m    357\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03m    Log a scikit-learn model as an MLflow artifact for the current run. Produces an MLflow Model\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[33;03m    containing the following flavors:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    424\u001b[39m \n\u001b[32m    425\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43msklearn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43msk_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43msk_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserialization_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserialization_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyfunc_predict_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpyfunc_predict_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/models/model.py:1297\u001b[39m, in \u001b[36mModel.log\u001b[39m\u001b[34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[39m\n\u001b[32m   1294\u001b[39m     \u001b[38;5;66;03m# mlflow_model is updated, rewrite the MLmodel file\u001b[39;00m\n\u001b[32m   1295\u001b[39m     mlflow_model.save(os.path.join(local_path, MLMODEL_FILE_NAME))\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_model_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[38;5;66;03m# If the model was previously identified as external, delete the tag because\u001b[39;00m\n\u001b[32m   1299\u001b[39m \u001b[38;5;66;03m# the model now has artifacts in MLflow Model format\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model.tags.get(MLFLOW_MODEL_IS_EXTERNAL, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m).lower() == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/tracking/client.py:5643\u001b[39m, in \u001b[36mMlflowClient.log_model_artifacts\u001b[39m\u001b[34m(self, model_id, local_dir)\u001b[39m\n\u001b[32m   5632\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_model_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m, local_dir: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5633\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5634\u001b[39m \u001b[33;03m    Upload a set of artifacts to the specified logged model.\u001b[39;00m\n\u001b[32m   5635\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5641\u001b[39m \u001b[33;03m        None\u001b[39;00m\n\u001b[32m   5642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_model_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/client.py:911\u001b[39m, in \u001b[36mTrackingServiceClient.log_model_artifacts\u001b[39m\u001b[34m(self, model_id, local_dir)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_model_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m, local_dir: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_artifact_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogged_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/store/artifact/local_artifact_repo.py:69\u001b[39m, in \u001b[36mLocalArtifactRepository.log_artifacts\u001b[39m\u001b[34m(self, local_dir, artifact_path)\u001b[39m\n\u001b[32m     65\u001b[39m artifact_dir = (\n\u001b[32m     66\u001b[39m     os.path.join(\u001b[38;5;28mself\u001b[39m.artifact_dir, artifact_path) \u001b[38;5;28;01mif\u001b[39;00m artifact_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.artifact_dir\n\u001b[32m     67\u001b[39m )\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(artifact_dir):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m shutil_copytree_without_file_permissions(local_dir, artifact_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/utils/file_utils.py:210\u001b[39m, in \u001b[36mmkdir\u001b[39m\u001b[34m(root, name)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.errno != errno.EEXIST \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(target):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m target\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mobile-item-recommendation/venv/lib/python3.13/site-packages/mlflow/utils/file_utils.py:207\u001b[39m, in \u001b[36mmkdir\u001b[39m\u001b[34m(root, name)\u001b[39m\n\u001b[32m    205\u001b[39m target = os.path.join(root, name) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m root\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.errno != errno.EEXIST \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(target):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:217\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:217\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "    \u001b[31m[... skipping similar frames: makedirs at line 217 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:217\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:227\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 30] Read-only file system: '/app'"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model_params=model_params, mlflow_experiment=MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8b8a8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-27 10:47:51.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1m================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:51.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPREDICTIONS ON TEST SET\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:51.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:51.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mMaking predictions on 75927 test samples...\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m\n",
      "================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mPREDICTION SUMMARY\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTotal test samples: 75927\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mActual positives (label=1): 261\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mPredicted positives (prediction=1): 463\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mTrue Positives: 32\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mFalse Positives: 431\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mFalse Negatives: 229\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mTrue Negatives: 75235\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1m\n",
      "Probability Statistics:\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1m  Min probability: 0.0007\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m  Max probability: 0.8972\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1m  Mean probability: 0.0377\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1m  Median probability: 0.0117\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1m\n",
      "================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mSAMPLE PREDICTIONS - TRUE POSITIVES (Correctly Predicted Buys)\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1m\n",
      "Showing 20 of 32 true positives\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1m\n",
      "================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSAMPLE PREDICTIONS - FALSE NEGATIVES (Missed Buys)\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1m================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1m\n",
      "Showing 20 of 229 false negatives (sorted by probability)\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1m\n",
      "================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mSAMPLE PREDICTIONS - FALSE POSITIVES (Incorrectly Predicted Buys)\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1m================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1m\n",
      "Showing 20 of 431 false positives (sorted by probability)\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1m\n",
      "================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:52.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mTOP PREDICTIONS BY PROBABILITY (All predictions sorted by buy_probability)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id    item_id  item_category  label  prediction  buy_probability\n",
      "2433    59409308  236514690          11991      1           1         0.552270\n",
      "3979    78679808  108523444           6648      1           1         0.545513\n",
      "6514    48643708  396027419          12553      1           1         0.656751\n",
      "6527    48643708  331484286           3690      1           1         0.519691\n",
      "10352  129163002  165101425           5993      1           1         0.569846\n",
      "10355  129163002  366320763           1686      1           1         0.564289\n",
      "15550   16296509  239395596          11991      1           1         0.508897\n",
      "20393   31356607  354525367           3942      1           1         0.569682\n",
      "24038   94518207  287824233          12326      1           1         0.689332\n",
      "25928  113587402  311685580           3942      1           1         0.653627\n",
      "28587   96141405  328312925           7572      1           1         0.523936\n",
      "29621   20229000  270664611           5480      1           1         0.522094\n",
      "30364   81799304  240505417           6417      1           1         0.694920\n",
      "31771   29010009   14136232          10662      1           1         0.659429\n",
      "36170   57892506  294167986           8792      1           1         0.839614\n",
      "36491   42385801  291385196           1963      1           1         0.823334\n",
      "36493   42385801  207333747           5065      1           1         0.638233\n",
      "36694    8646903  305613858           3660      1           1         0.763107\n",
      "43274   29745002  146714345           3925      1           1         0.773062\n",
      "46585   61739600   50335517           4008      1           1         0.651559\n",
      "         user_id    item_id  item_category  label  prediction  buy_probability\n",
      "863     22546008  182361099          11492      1           0         0.489061\n",
      "2431    59409308   56746521          11991      1           0         0.486748\n",
      "28498   50738506  261965119           3942      1           0         0.484719\n",
      "9631   104750003  213012452          10662      1           0         0.483492\n",
      "6003    57227902   88474745          10412      1           0         0.483467\n",
      "62712   79640802  181151182          10236      1           0         0.471012\n",
      "35086   93910904  170522202           4245      1           0         0.462069\n",
      "28393    3973208   83021826           6794      1           0         0.453551\n",
      "26459  101246500  294167986           8792      1           0         0.451274\n",
      "7758    65079708  294095847            997      1           0         0.440181\n",
      "35291   28007106   86865172           3321      1           0         0.437160\n",
      "38992   86996304  306931036          10734      1           0         0.434540\n",
      "69531   29272809  250149265          10063      1           0         0.433800\n",
      "40188  109915708   61883795          11406      1           0         0.432778\n",
      "68606    8547606  349051058          12304      1           0         0.425643\n",
      "75610   20571103  146073997           7350      1           0         0.417084\n",
      "39499   51281000   61042684           3368      1           0         0.413353\n",
      "33272   56524902   98791680           9614      1           0         0.404874\n",
      "5164   138830506  347142962          13042      1           0         0.403108\n",
      "58989   79665100  245145278          12067      1           0         0.394893\n",
      "         user_id    item_id  item_category  label  prediction  buy_probability\n",
      "23332  140196509  315119130          12096      0           1         0.897175\n",
      "9275   102961104   79329525          10895      0           1         0.896901\n",
      "17736   63014409    7621463           9307      0           1         0.891091\n",
      "20586   65862109   93657183           3942      0           1         0.889717\n",
      "47256    1659408  151705313          12158      0           1         0.888716\n",
      "65462  113844402  401890785           9614      0           1         0.880069\n",
      "9281   102961104  114450090          14047      0           1         0.869505\n",
      "69096     427500  113202187           9334      0           1         0.869382\n",
      "17759   63014409   89506164           9232      0           1         0.869221\n",
      "64558   55225001  368321478          10431      0           1         0.859529\n",
      "58208    6223506  318556386          11991      0           1         0.852175\n",
      "17747   63014409   25810762          10412      0           1         0.852167\n",
      "33921   82025305  395881050           5993      0           1         0.846096\n",
      "60714   43414305  134867773           2284      0           1         0.839578\n",
      "36167   57892506  249480376           5002      0           1         0.839252\n",
      "36171   57892506  289454952           5002      0           1         0.828338\n",
      "28445   61641806    5071576           1029      0           1         0.822854\n",
      "41527  125954604  201585168           2284      0           1         0.819669\n",
      "61100   36718105  269961859          12630      0           1         0.816378\n",
      "51136   54126507  369569155           3960      0           1         0.812348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-27 10:47:52.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:53.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1m\n",
      "================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:53.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mPREDICTIONS FOR ACTUAL BUYERS (label=1) - Sorted by Probability\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:53.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1m================================================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id    item_id  item_category  label  prediction  buy_probability\n",
      "23332  140196509  315119130          12096      0           1         0.897175\n",
      "9275   102961104   79329525          10895      0           1         0.896901\n",
      "17736   63014409    7621463           9307      0           1         0.891091\n",
      "20586   65862109   93657183           3942      0           1         0.889717\n",
      "47256    1659408  151705313          12158      0           1         0.888716\n",
      "65462  113844402  401890785           9614      0           1         0.880069\n",
      "9281   102961104  114450090          14047      0           1         0.869505\n",
      "69096     427500  113202187           9334      0           1         0.869382\n",
      "17759   63014409   89506164           9232      0           1         0.869221\n",
      "64558   55225001  368321478          10431      0           1         0.859529\n",
      "58208    6223506  318556386          11991      0           1         0.852175\n",
      "17747   63014409   25810762          10412      0           1         0.852167\n",
      "33921   82025305  395881050           5993      0           1         0.846096\n",
      "47896    9439407  138880554           9205      1           1         0.840472\n",
      "36170   57892506  294167986           8792      1           1         0.839614\n",
      "60714   43414305  134867773           2284      0           1         0.839578\n",
      "36167   57892506  249480376           5002      0           1         0.839252\n",
      "36171   57892506  289454952           5002      0           1         0.828338\n",
      "36491   42385801  291385196           1963      1           1         0.823334\n",
      "28445   61641806    5071576           1029      0           1         0.822854\n",
      "41527  125954604  201585168           2284      0           1         0.819669\n",
      "61100   36718105  269961859          12630      0           1         0.816378\n",
      "51136   54126507  369569155           3960      0           1         0.812348\n",
      "583     90716904   18170459           3660      0           1         0.810212\n",
      "6945    63861902  254520974           7390      0           1         0.807129\n",
      "58032   88574400  276458769           1032      1           1         0.796867\n",
      "62613   85049204  288185042           2348      0           1         0.796270\n",
      "42450   42984106  182978815           6874      0           1         0.794717\n",
      "9270   102961104   28286261          10356      0           1         0.792818\n",
      "8747   130458109    7589444           6417      0           1         0.789014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-27 10:47:54.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1m\n",
      "================================================================================\u001b[0m\n",
      "\u001b[32m2025-11-27 10:47:54.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mPREDICTIONS COMPLETE\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id    item_id  item_category  label  prediction  buy_probability\n",
      "47896    9439407  138880554           9205      1           1         0.840472\n",
      "36170   57892506  294167986           8792      1           1         0.839614\n",
      "36491   42385801  291385196           1963      1           1         0.823334\n",
      "58032   88574400  276458769           1032      1           1         0.796867\n",
      "46828    8884009  355024510           3368      1           1         0.785616\n",
      "43274   29745002  146714345           3925      1           1         0.773062\n",
      "54266   29093004  388669975           2029      1           1         0.771537\n",
      "36694    8646903  305613858           3660      1           1         0.763107\n",
      "30364   81799304  240505417           6417      1           1         0.694920\n",
      "24038   94518207  287824233          12326      1           1         0.689332\n",
      "31771   29010009   14136232          10662      1           1         0.659429\n",
      "6514    48643708  396027419          12553      1           1         0.656751\n",
      "25928  113587402  311685580           3942      1           1         0.653627\n",
      "46585   61739600   50335517           4008      1           1         0.651559\n",
      "56470   38763309  233575611          10735      1           1         0.651362\n",
      "71185   10252103   24696776           3660      1           1         0.638666\n",
      "36493   42385801  207333747           5065      1           1         0.638233\n",
      "10352  129163002  165101425           5993      1           1         0.569846\n",
      "20393   31356607  354525367           3942      1           1         0.569682\n",
      "69435   14075202  112114135           8744      1           1         0.568761\n",
      "10355  129163002  366320763           1686      1           1         0.564289\n",
      "61046  106502507  208588130           8561      1           1         0.563714\n",
      "72930   33271407  298391440           5480      1           1         0.561149\n",
      "54268   29093004   61557472          10431      1           1         0.560829\n",
      "2433    59409308  236514690          11991      1           1         0.552270\n",
      "3979    78679808  108523444           6648      1           1         0.545513\n",
      "62379   82510503    7932319           9569      1           1         0.543560\n",
      "28587   96141405  328312925           7572      1           1         0.523936\n",
      "29621   20229000  270664611           5480      1           1         0.522094\n",
      "6527    48643708  331484286           3690      1           1         0.519691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-27 10:47:54.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Print predictions on test_df\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"PREDICTIONS ON TEST SET\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "# Load the best model from MLflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_name = \"mobile_item_rec_model\"\n",
    "\n",
    "loaded_model = mlflow.sklearn.load_model(\"/Users/duc.tran/mobile-item-recommendation/notebooks/mlruns/916499331672477983/models/m-92fb2d2b3d1f49ed8f728a6635120e1e/artifacts/\")\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_parquet(TEST_PATH)\n",
    "drop_cols = ['user_id', 'item_id', 'item_category', 'label']\n",
    "feature_cols = [c for c in test_df.columns if c not in drop_cols]\n",
    "\n",
    "# Make predictions\n",
    "logger.info(f\"Making predictions on {len(test_df)} test samples...\")\n",
    "test_predictions = loaded_model.predict(test_df[feature_cols])\n",
    "test_probabilities = loaded_model.predict_proba(test_df[feature_cols])[:, 1]\n",
    "\n",
    "# Add predictions to test_df\n",
    "test_df['prediction'] = test_predictions\n",
    "test_df['buy_probability'] = test_probabilities\n",
    "\n",
    "# Print summary statistics\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"PREDICTION SUMMARY\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(f\"Total test samples: {len(test_df)}\")\n",
    "logger.info(f\"Actual positives (label=1): {test_df['label'].sum()}\")\n",
    "logger.info(f\"Predicted positives (prediction=1): {test_predictions.sum()}\")\n",
    "logger.info(f\"True Positives: {((test_df['label'] == 1) & (test_predictions == 1)).sum()}\")\n",
    "logger.info(f\"False Positives: {((test_df['label'] == 0) & (test_predictions == 1)).sum()}\")\n",
    "logger.info(f\"False Negatives: {((test_df['label'] == 1) & (test_predictions == 0)).sum()}\")\n",
    "logger.info(f\"True Negatives: {((test_df['label'] == 0) & (test_predictions == 0)).sum()}\")\n",
    "\n",
    "logger.info(f\"\\nProbability Statistics:\")\n",
    "logger.info(f\"  Min probability: {test_probabilities.min():.4f}\")\n",
    "logger.info(f\"  Max probability: {test_probabilities.max():.4f}\")\n",
    "logger.info(f\"  Mean probability: {test_probabilities.mean():.4f}\")\n",
    "logger.info(f\"  Median probability: {np.median(test_probabilities):.4f}\")\n",
    "\n",
    "# Show samples of predictions\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"SAMPLE PREDICTIONS - TRUE POSITIVES (Correctly Predicted Buys)\")\n",
    "logger.info(\"=\" * 80)\n",
    "true_positives = test_df[(test_df['label'] == 1) & (test_predictions == 1)]\n",
    "if len(true_positives) > 0:\n",
    "    print(true_positives[['user_id', 'item_id', 'item_category', 'label', 'prediction', 'buy_probability']].head(20))\n",
    "    logger.info(f\"\\nShowing {min(20, len(true_positives))} of {len(true_positives)} true positives\")\n",
    "else:\n",
    "    logger.info(\"No true positives found\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"SAMPLE PREDICTIONS - FALSE NEGATIVES (Missed Buys)\")\n",
    "logger.info(\"=\" * 80)\n",
    "false_negatives = test_df[(test_df['label'] == 1) & (test_predictions == 0)]\n",
    "if len(false_negatives) > 0:\n",
    "    # Sort by probability to see which ones were close\n",
    "    false_negatives_sorted = false_negatives.sort_values('buy_probability', ascending=False)\n",
    "    print(false_negatives_sorted[['user_id', 'item_id', 'item_category', 'label', 'prediction', 'buy_probability']].head(20))\n",
    "    logger.info(f\"\\nShowing {min(20, len(false_negatives))} of {len(false_negatives)} false negatives (sorted by probability)\")\n",
    "else:\n",
    "    logger.info(\"No false negatives found\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"SAMPLE PREDICTIONS - FALSE POSITIVES (Incorrectly Predicted Buys)\")\n",
    "logger.info(\"=\" * 80)\n",
    "false_positives = test_df[(test_df['label'] == 0) & (test_predictions == 1)]\n",
    "if len(false_positives) > 0:\n",
    "    # Sort by probability to see which ones had high confidence\n",
    "    false_positives_sorted = false_positives.sort_values('buy_probability', ascending=False)\n",
    "    print(false_positives_sorted[['user_id', 'item_id', 'item_category', 'label', 'prediction', 'buy_probability']].head(20))\n",
    "    logger.info(f\"\\nShowing {min(20, len(false_positives))} of {len(false_positives)} false positives (sorted by probability)\")\n",
    "else:\n",
    "    logger.info(\"No false positives found\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"TOP PREDICTIONS BY PROBABILITY (All predictions sorted by buy_probability)\")\n",
    "logger.info(\"=\" * 80)\n",
    "top_predictions = test_df.nlargest(30, 'buy_probability')\n",
    "print(top_predictions[['user_id', 'item_id', 'item_category', 'label', 'prediction', 'buy_probability']])\n",
    "\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"PREDICTIONS FOR ACTUAL BUYERS (label=1) - Sorted by Probability\")\n",
    "logger.info(\"=\" * 80)\n",
    "actual_buyers = test_df[test_df['label'] == 1].sort_values('buy_probability', ascending=False)\n",
    "print(actual_buyers[['user_id', 'item_id', 'item_category', 'label', 'prediction', 'buy_probability']].head(30))\n",
    "\n",
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"PREDICTIONS COMPLETE\")\n",
    "logger.info(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
